        out[c] = else:
    raw = input("Month # Notes:
# - Keys that are missing are not comparable; they are flagged separately.
# - If duplicates exist, diffs use the first occurrence per key, and duplicates are flagged for review.

def _dedupe_first(df: pd.DataFrame) -> pd.DataFrame:
    # For diffing only: select first row per key (does not modify the original dataset)
    out = df.copy()
    out = out.sort_index()
    out = out.dropna(subset=[COMPARE_KEY]).copy()  # diffing requires a key; missing keys are handled as validation
    return out.drop_duplicates(subset=[COMPARE_KEY], keep="first").copy()

diffs = {
    "new_customers": pd.DataFrame(columns=OBJ_COLS),
    "removed_customers": pd.DataFrame(columns=OBJ_COLS),
    "changed_summary": pd.DataFrame(columns=[COMPARE_KEY, "Changed Fields"]),
    "changed_detail": pd.DataFrame(columns=[COMPARE_KEY, "Field", "Previous Value", "Current Value"]),
    "category_transition_matrix": pd.DataFrame(),
    "category_moved_rows": pd.DataFrame(columns=[COMPARE_KEY, "Category_prev", "Category_curr"]),
    "comparison_warnings": pd.DataFrame(columns=["Warning", "Count"]),
}

if df_prev is not None:
    prev_d = _dedupe_first(df_prev)
    curr_d = _dedupe_first(df_current)

    # Build outer join by key
    merged = prev_d.merge(
        curr_d,
        on=COMPARE_KEY,
        how="outer",
        suffixes=("_prev", "_curr"),
        indicator=True
    )

    compare_cols = [c for c in OBJ_COLS if c != COMPARE_KEY]

    # New / removed
    new_df = merged.loc[merged["_merge"] == "right_only", [COMPARE_KEY] + [f"{c}_curr" for c in compare_cols]].copy()
    new_df.columns = [COMPARE_KEY] + compare_cols
    diffs["new_customers"] = new_df[OBJ_COLS].copy()

    rem_df = merged.loc[merged["_merge"] == "left_only", [COMPARE_KEY] + [f"{c}_prev" for c in compare_cols]].copy()
    rem_df.columns = [COMPARE_KEY] + compare_cols
    diffs["removed_customers"] = rem_df[OBJ_COLS].copy()

    # Changed (common keys)
    common = merged.loc[merged["_merge"] == "both"].copy()

    change_rows = []
    for col in compare_cols:
        eq = _values_equal(common[f"{col}_prev"], common[f"{col}_curr"], col=col)
        mask = ~eq
        if mask.any():
            tmp = common.loc[mask, [COMPARE_KEY, f"{col}_prev", f"{col}_curr"]].copy()
            tmp["Field"] = col
            tmp.rename(columns={
                f"{col}_prev": "Previous Value",
                f"{col}_curr": "Current Value"
            }, inplace=True)
            change_rows.append(tmp[[COMPARE_KEY, "Field", "Previous Value", "Current Value"]])

    changed_detail = pd.concat(change_rows, ignore_index=True) if change_rows else diffs["changed_detail"].copy()

    # Summary per key
    if len(changed_detail):
        changed_summary = (
            changed_detail.groupby(COMPARE_KEY)["Field"]
            .apply(lambda s: ", ".join(sorted(set(s))))
            .reset_index()
            .rename(columns={"Field": "Changed Fields"})
        )
    else:
        changed_summary = diffs["changed_summary"].copy()

    diffs["changed_detail"] = changed_detail.sort_values([COMPARE_KEY, "Field"]).reset_index(drop=True)
    diffs["changed_summary"] = changed_summary.sort_values([COMPARE_KEY]).reset_index(drop=True)

    # Category transitions
    trans = common[[COMPARE_KEY, "Category_prev", "Category_curr"]].copy()
    trans["Category_prev"] = trans["Category_prev"].astype("string").fillna("(Blank)").str.strip()
    trans["Category_curr"] = trans["Category_curr"].astype("string").fillna("(Blank)").str.strip()

    mat = (
        trans.pivot_table(index="Category_prev", columns="Category_curr", values=COMPARE_KEY, aggfunc="count", fill_value=0)
        .reset_index()
    )
    diffs["category_transition_matrix"] = mat

    moved = trans.loc[trans["Category_prev"] != trans["Category_curr"]].copy()
    diffs["category_moved_rows"] = moved.sort_values([COMPARE_KEY]).reset_index(drop=True)

    # Comparison warnings
    warnings = []
    warnings.append({"Warning": "Duplicate keys in current (rows)", "Count": int(len(dups_current))})
    warnings.append({"Warning": "Duplicate keys in previous (rows)", "Count": int(len(dups_prev))})
    warnings.append({"Warning": "Missing keys in current (rows)", "Count": int(df_current[COMPARE_KEY].isna().sum())})
    warnings.append({"Warning": "Missing keys in previous (rows)", "Count": int(df_prev[COMPARE_KEY].isna().sum())})
    diffs["comparison_warnings"] = pd.DataFrame(warnings)

    print("Diff summary:")
    print(" - New customers:", len(diffs["new_customers"]))
    print(" - Removed customers:", len(diffs["removed_customers"]))
    print(" - Changed customers:", len(diffs["changed_summary"]))
    print(" - Category moves:", len(diffs["category_moved_rows"]))
else:
    print("Diffs skipped (no previous dataset).")


Cell 14 — Running validation rules (flag-only; no corrections)
# Validation rules (flag only):
# 1) NO OF CUST must be numeric and >= 1 and not missing
# 2) Category must exist and be one of: Managed, Shadow, Resolved
# 3) If Category == Managed -> Managed Status must not be blank/N/A
# 4) Resolution Date: if Resolved -> must exist; else -> should be blank
# 5) Resolution Type missing/NA -> flagged (for churn reporting bucket)
# 6) Required fields not blank: Customer Name, Category, DoD, Business, Total Relationship Name
# 7) RM Name may be NA only if Resolved and Resolution Date older than 12 months (relative to REPORT_DATE)
# 8) LCL_CUST_ID missing flagged
# 9) Duplicate LCL_CUST_ID flagged
# 10) Distinct-values scan produced separately

def run_validations(df: pd.DataFrame, report_date: pd.Timestamp) -> Dict[str, pd.DataFrame]:
    issues: Dict[str, pd.DataFrame] = {}

    # R01 Mandatory fields not blank
    mandatory = ["Customer Name", "Category", "DoD", "Business", "Total Relationship Name"]
    mask = pd.Series(False, index=df.index)
    for c in mandatory:
        mask |= _blank_text_mask(df[c])
    issues["R01_Missing_Required_Fields"] = df.loc[mask, OBJ_COLS].copy()

    # R02 Category validity
    cat = df["Category"].astype("string")
    issues["R02_Invalid_Category"] = df.loc[~cat.isin(list(ALLOWED_CATEGORIES)), OBJ_COLS].copy()

    # R03 NO OF CUST numeric >= 1 and not missing
    noc = pd.to_numeric(df["NO OF CUST"], errors="coerce")
    issues["R03_Invalid_NO_OF_CUST"] = df.loc[noc.isna() | (noc < 1), OBJ_COLS].copy()

    # R04 Managed status required for Managed category
    managed = (df["Category"].astype("string") == "Managed")
    ms_blank = _blank_text_mask(df["Managed Status"]) | (df["Managed Status"].astype("string").str.strip().str.upper() == "N/A")
    issues["R04_Managed_Status_Missing"] = df.loc[managed & ms_blank, OBJ_COLS].copy()

    # R05 Resolution Date logic
    resolved = (df["Category"].astype("string") == "Resolved")
    res_date = df["Resolution Date"]
    issues["R05A_Resolved_Missing_Resolution_Date"] = df.loc[resolved & res_date.isna(), OBJ_COLS].copy()
    issues["R05B_NonResolved_Has_Resolution_Date"] = df.loc[(~resolved) & res_date.notna(), OBJ_COLS].copy()

    # R06 Resolution Type missing (churn bucket)
    rt_blank = _blank_text_mask(df["Resolution Type"]) | (df["Resolution Type"].astype("string").str.strip().str.upper() == "N/A")
    issues["R06_Resolution_Type_Missing_For_Churn"] = df.loc[rt_blank, OBJ_COLS].copy()

    # R07 RM Name logic (allowed missing only if resolved AND resolution date older than 12 months)
    rm_blank = _blank_text_mask(df["RM Name"]) | (df["RM Name"].astype("string").str.strip().str.upper() == "N/A")
    cutoff = report_date - pd.DateOffset(months=12)
    # Missing RM is allowed only when resolved & res_date < cutoff
    allowed_missing_rm = resolved & res_date.notna() & (res_date < cutoff)
    issues["R07_RM_Name_Suspect_Missing"] = df.loc[rm_blank & ~allowed_missing_rm, OBJ_COLS].copy()

    # R08 Missing key
    issues["R08_Missing_LCL_CUST_ID"] = df.loc[df[COMPARE_KEY].isna(), OBJ_COLS].copy()

    # R09 Duplicate key
    k_notna = df[COMPARE_KEY].notna()
    dup = df.loc[k_notna].duplicated(subset=[COMPARE_KEY], keep=False)
    issues["R09_Duplicate_LCL_CUST_ID"] = df.loc[k_notna].loc[dup, OBJ_COLS].copy()

    return issues

issues = run_validations(df_current, report_date=REPORT_DATE)

validation_summary = (
    pd.DataFrame([{"Rule": k, "Issue Rows": int(len(v))} for k, v in issues.items()])
    .sort_values(["Issue Rows", "Rule"], ascending=[False, True])
    .reset_index(drop=True)
)

print("Validation summary (rows flagged):")
print(validation_summary)


Cell 15 — Distinct-values scans (key categorical fields)
def distinct_values_scan(df: pd.DataFrame, fields: List[str], top_n: int = 200) -> pd.DataFrame:
    rows = []
    for f in fields:
        s = df[f].astype("string").fillna("(Blank)").str.strip()
        vc = s.value_counts(dropna=False).head(top_n)
        for val, cnt in vc.items():
            rows.append({"Field": f, "Value": val, "Count": int(cnt)})
    return pd.DataFrame(rows)

distinct_scan = distinct_values_scan(
    df_current,
    fields=["Category", "Managed Status", "Resolution Type", "DoD", "Business"],
    top_n=200
)

print("Distinct-values scan rows:", len(distinct_scan))


Cell 16 — Building the customer-focused review list (one row per LCL_CUST_ID; union of keys)
def _aggregate_validation_flags_by_key(issues_dict: Dict[str, pd.DataFrame]) -> pd.DataFrame:
    rows = []
    for rule_name, df_issue in issues_dict.items():
        if df_issue is None or len(df_issue) == 0:
            continue
        tmp = df_issue[[COMPARE_KEY]].copy()
        tmp["Rule"] = rule_name
        rows.append(tmp)

    if not rows:
        return pd.DataFrame(columns=[COMPARE_KEY, "Validation Flags"])

    out = pd.concat(rows, ignore_index=True)
    out = out.dropna(subset=[COMPARE_KEY]).copy()
    out[COMPARE_KEY] = pd.to_numeric(out[COMPARE_KEY], errors="coerce").astype("Int64")

    return (
        out.groupby(COMPARE_KEY)["Rule"]
        .apply(lambda s: ", ".join(sorted(set(s))))
        .reset_index()
        .rename(columns={"Rule": "Validation Flags"})
    )

flags_by_key = _aggregate_validation_flags_by_key(issues)

# Base rows for current and previous (deduped first for a single-row representation)
curr_first = _dedupe_first(df_current) if len(df_current) else df_current.copy()
prev_first = _dedupe_first(df_prev) if df_prev is not None and len(df_prev) else (df_prev.copy() if df_prev is not None else None)

# Universe of keys (exclude NA; those are handled via validation tabs)
keys_curr = set(curr_first[COMPARE_KEY].dropna().astype("Int64").tolist())
keys_prev = set(prev_first[COMPARE_KEY].dropna().astype("Int64").tolist()) if prev_first is not None else set()
all_keys = sorted(keys_curr.union(keys_prev))

focus = pd.DataFrame({COMPARE_KEY: pd.Series(all_keys, dtype="Int64")})

# Bring forward representative attributes (current preferred; fallback to previous for removed)
focus = focus.merge(
    curr_first[[COMPARE_KEY, "Customer Name", "Category", "Month Added", "NO OF CUST", "Managed Status", "DoD", "Business"]].copy(),
    on=COMPARE_KEY, how="left"
)

if prev_first is not None:
    focus = focus.merge(
        prev_first[[COMPARE_KEY, "Customer Name", "Category", "Month Added", "NO OF CUST", "Managed Status", "DoD", "Business"]].copy(),
        on=COMPARE_KEY, how="left", suffixes=("", "_prev_fallback")
    )
    # Fill missing current attributes with previous (for removed customers)
    for col in ["Customer Name", "Category", "Month Added", "NO OF CUST", "Managed Status", "DoD", "Business"]:
        focus[col] = focus[col].combine_first(focus[f"{col}_prev_fallback"])
        focus.drop(columns=[f"{col}_prev_fallback"], inplace=True)

# Flags + diffs
focus = focus.merge(flags_by_key, on=COMPARE_KEY, how="left")
focus["Validation Flags"] = focus["Validation Flags"].fillna("")

if df_prev is not None:
    focus = focus.merge(diffs["changed_summary"], on=COMPARE_KEY, how="left")
    focus["Changed Fields"] = focus["Changed Fields"].fillna("")

    moved_keys = set(diffs["category_moved_rows"][COMPARE_KEY].dropna().astype("Int64").tolist())
    focus["Category Moved (Y/N)"] = focus[COMPARE_KEY].apply(lambda k: "Y" if k in moved_keys else "N")

    new_keys = set(diffs["new_customers"][COMPARE_KEY].dropna().astype("Int64").tolist())
    removed_keys = set(diffs["removed_customers"][COMPARE_KEY].dropna().astype("Int64").tolist())
    focus["New (Y/N)"] = focus[COMPARE_KEY].apply(lambda k: "Y" if k in new_keys else "N")
    focus["Removed (Y/N)"] = focus[COMPARE_KEY].apply(lambda k: "Y" if k in removed_keys else "N")
else:
    focus["Changed Fields"] = ""
    focus["Category Moved (Y/N)"] = "N/A"
    focus["New (Y/N)"] = "N/A"
    focus["Removed (Y/N)"] = "N/A"

# Priority score: validation > category move > other changes > new > removed
def _priority_score(r) -> int:
    score = 0
    if str(r.get("Validation Flags", "")).strip():
        score += 100
    if str(r.get("Category Moved (Y/N)", "")).strip() == "Y":
        score += 50
    if str(r.get("Changed Fields", "")).strip():
        score += 20
    if str(r.get("New (Y/N)", "")).strip() == "Y":
        score += 10
    if str(r.get("Removed (Y/N)", "")).strip() == "Y":
        score += 30
    return score

focus["Priority Score"] = focus.apply(_priority_score, axis=1)

# Ordering and final column selection per requirements
focus = focus.sort_values(["Priority Score", "Customer Name"], ascending=[False, True], na_position="last").reset_index(drop=True)

FOCUS_COLS = [
    "LCL_CUST_ID",
    "Customer Name",
    "Category",
    "Month Added",
    "NO OF CUST",
    "Managed Status",
    "DoD",
    "Business",
    "Validation Flags",
    "Changed Fields",
    "Category Moved (Y/N)",
    "New (Y/N)",
    "Removed (Y/N)",
    "Priority Score",
]

focus = focus[FOCUS_COLS].copy()

print("Focus list rows:", len(focus))
print(focus.head(10))


Cell 17 — Building 'At a Glance' metrics (front page of review pack)
at_a_glance_rows = [
    {"Metric": "Current rows", "Value": int(len(df_current))},
    {"Metric": "Previous rows", "Value": int(len(df_prev)) if df_prev is not None else 0},
    {"Metric": "Current missing LCL_CUST_ID rows", "Value": int(df_current[COMPARE_KEY].isna().sum())},
    {"Metric": "Current duplicate LCL_CUST_ID rows", "Value": int(len(dups_current))},
    {"Metric": "Previous duplicate LCL_CUST_ID rows", "Value": int(len(dups_prev)) if df_prev is not None else 0},
]

# Validation hits by rule (row counts)
for _, r in validation_summary.iterrows():
    if int(r["Issue Rows"]) > 0:
        at_a_glance_rows.append({"Metric": f"Validation rows: {r['Rule']}", "Value": int(r["Issue Rows"])})

# Diff counts
if df_prev is not None:
    at_a_glance_rows += [
        {"Metric": "New customers (by key)", "Value": int(len(diffs["new_customers"]))},
        {"Metric": "Removed customers (by key)", "Value": int(len(diffs["removed_customers"]))},
        {"Metric": "Changed customers (by key)", "Value": int(len(diffs["changed_summary"]))},
        {"Metric": "Category moves (prev != curr)", "Value": int(len(diffs["category_moved_rows"]))},
    ]

at_a_glance = pd.DataFrame(at_a_glance_rows)
at_a_glance


Cell 18 — Finalizing review-pack tables (ensure required tabs exist even when diffs are skipped)
# 02_New_Customers
tab_new = diffs["new_customers"].copy() if df_prev is not None else pd.DataFrame(columns=OBJ_COLS)

# 03_Removed_Customers
tab_removed = diffs["removed_customers"].copy() if df_prev is not None else pd.DataFrame(columns=OBJ_COLS)

# 04_Changed_Summary
tab_changed_summary = diffs["changed_summary"].copy() if df_prev is not None else pd.DataFrame(columns=[COMPARE_KEY, "Changed Fields"])

# 05_Changed_Detail
tab_changed_detail = diffs["changed_detail"].copy() if df_prev is not None else pd.DataFrame(columns=[COMPARE_KEY, "Field", "Previous Value", "Current Value"])

# 06_Category_Transitions
tab_cat_trans = diffs["category_transition_matrix"].copy() if df_prev is not None else pd.DataFrame()

# 07_Category_Moved_Rows — expand with customer names (prev/current) when available
if df_prev is not None and len(diffs["category_moved_rows"]):
    prev_first = _dedupe_first(df_prev)
    curr_first = _dedupe_first(df_current)
    cat_moved = diffs["category_moved_rows"].merge(
        prev_first[[COMPARE_KEY, "Customer Name"]].rename(columns={"Customer Name": "Customer Name (Prev)"}),
        on=COMPARE_KEY, how="left"
    ).merge(
        curr_first[[COMPARE_KEY, "Customer Name"]].rename(columns={"Customer Name": "Customer Name (Curr)"}),
        on=COMPARE_KEY, how="left"
    )
    tab_cat_moved = cat_moved[[COMPARE_KEY, "Customer Name (Prev)", "Customer Name (Curr)", "Category_prev", "Category_curr"]].copy()
else:
    tab_cat_moved = pd.DataFrame(columns=[COMPARE_KEY, "Customer Name (Prev)", "Customer Name (Curr)", "Category_prev", "Category_curr"])

# 08_Validation_Summary
tab_val_summary = validation_summary.copy()

# Validation drill-down tabs
validation_tabs = issues.copy()

# Distinct scan
tab_distinct = distinct_scan.copy()

# Comparison warnings
tab_compare_warn = diffs["comparison_warnings"].copy() if df_prev is not None else pd.DataFrame(columns=["Warning", "Count"])


Cell 19 — Writing the Excel review pack (single workbook)
review_pack_path = os.path.join(OUTPUT_DIR, f"SCU_MSR_ReviewPack_{run_date}.xlsx")

def _safe_sheet_name(name: str) -> str:
    # Excel sheet max length: 31
    # Remove invalid chars: : \ / ? * [ ]
    name = re.sub(r"[:\\/\?\*\[\]]", "_", name)
    return name[:31]

with pd.ExcelWriter(review_pack_path, engine="openpyxl") as writer:
    at_a_glance.to_excel(writer, sheet_name="00_At_A_Glance", index=False)
    focus.to_excel(writer, sheet_name="01_Customers_To_Focus", index=False)

    tab_new.to_excel(writer, sheet_name="02_New_Customers", index=False)
    tab_removed.to_excel(writer, sheet_name="03_Removed_Customers", index=False)
    tab_changed_summary.to_excel(writer, sheet_name="04_Changed_Summary", index=False)
    tab_changed_detail.to_excel(writer, sheet_name="05_Changed_Detail", index=False)

    # Category transitions / moved rows
    if isinstance(tab_cat_trans, pd.DataFrame) and len(tab_cat_trans):
        tab_cat_trans.to_excel(writer, sheet_name="06_Category_Transitions", index=False)
    else:
        pd.DataFrame().to_excel(writer, sheet_name="06_Category_Transitions", index=False)

    tab_cat_moved.to_excel(writer, sheet_name="07_Category_Moved_Rows", index=False)

    # Validation summary
    tab_val_summary.to_excel(writer, sheet_name="08_Validation_Summary", index=False)

    # Distinct scan + comparison warnings
    tab_distinct.to_excel(writer, sheet_name="09_Distinct_Values", index=False)
    tab_compare_warn.to_excel(writer, sheet_name="10_Comparison_Warnings", index=False)

    # One tab per validation rule
    for rule, df_issue in validation_tabs.items():
        sheet = _safe_sheet_name(f"VAL_{rule}")
        df_issue.to_excel(writer, sheet_name=sheet, index=False)

    # Meta
    meta = pd.DataFrame([
        {"Item": "Run date", "Value": run_date},
        {"Item": "Run timestamp", "Value": datetime.now().isoformat(timespec="seconds")},
        {"Item": "Current file", "Value": CURRENT_PATH},
        {"Item": "Previous file", "Value": PREV_PATH if PREV_PATH else "N/A"},
        {"Item": "Previous sheet selected", "Value": prev_sheet if prev_sheet else "N/A"},
        {"Item": "Report date used", "Value": str(REPORT_DATE.date())},
        {"Item": "Current rows", "Value": int(len(df_current))},
        {"Item": "Previous rows", "Value": int(len(df_prev)) if df_prev is not None else 0},
        {"Item": "Schema version", "Value": "SharePoint_A_N_14cols_v1"},
    ])
    meta.to_excel(writer, sheet_name="99_Meta", index=False)

print("Review pack written:", review_pack_path)


Cell 20 — Preview: highest priority items
preview_cols = [
    "LCL_CUST_ID", "Customer Name", "Category", "Priority Score",
    "Validation Flags", "Changed Fields", "Category Moved (Y/N)", "New (Y/N)", "Removed (Y/N)"
]
focus[preview_cols].head(15)


